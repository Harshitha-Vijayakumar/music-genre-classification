{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to be included\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaggle API command\n",
    "!kaggle datasets download -d andradaolteanu/gtzan-dataset-music-genre-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzipping the file that was downloaded\n",
    "import zipfile\n",
    "\n",
    "file_path = '/content/gtzan-dataset-music-genre-classification.zip'\n",
    "\n",
    "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing the images into 180x180 size and converting them to tensor\n",
    "transform = transforms.Compose([transforms.Resize((180, 180)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/Data/images_original'\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into 70, 20 and 10 for training, validation and testing\n",
    "train, val = int(len(dataset)*0.7), int(len(dataset)*0.2)\n",
    "test = int(len(dataset) - train - val)\n",
    "train_data, val_data, test_data = random_split(dataset, [train, val, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "samples.shape, labels.shape #To check the input tensor size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(6):\n",
    "  plt.subplot(2, 3, i+1)\n",
    "  plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show() #Viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "def train_model(epochs, train_loader, model, loss_function, optimizer):\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "      #forward\n",
    "      output = model(imgs)\n",
    "      loss = loss_fn(output, labels)\n",
    "\n",
    "      #backward\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    print(f'epoch{epoch+1}/{epochs}, loss = {loss.item():.4f}')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of the validation set\n",
    "def eval1(model):\n",
    "  correct = 0\n",
    "  total = 0.0\n",
    "  with torch.no_grad():\n",
    "      for images, labels in val_loader:\n",
    "          images = images.view(images.size(0), -1)  # Flatten the images\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  val_accuracy = correct / total * 100\n",
    "  return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of the testing set\n",
    "def eval2(model):\n",
    "  correct = 0\n",
    "  total = 0.0\n",
    "  with torch.no_grad():\n",
    "      for images, labels in test_loader:\n",
    "          images = images.view(images.size(0), -1)  # Flatten the images\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  test_accuracy = correct / total * 100\n",
    "  return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully connected neural network with 2 hidden layers\n",
    "input_size = 180*180*3\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 256\n",
    "output_size = len(dataset.classes)\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net1, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net1(input_size, hidden_size1, hidden_size2, output_size)\n",
    "model1 = model1.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss() #Loss function - Cross entropy\n",
    "optimizer = optim.Adam(model1.parameters()) #Optimizer - Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 50 epochs\n",
    "model1 = train_model(50, train_loader, model1, loss_fn, optimizer)\n",
    "val_accuracy = eval1(model1)\n",
    "print(f'Accuracy on Validation set: {val_accuracy}')\n",
    "test_accuracy = eval2(model1)\n",
    "print(f'Accuracy on Test set: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 100 epochs\n",
    "model1 = train_model(100, train_loader, model1, loss_fn, optimizer)\n",
    "val_accuracy = eval1(model1)\n",
    "print(f'Accuracy on Validation set: {val_accuracy}')\n",
    "test_accuracy = eval2(model1)\n",
    "print(f'Accuracy on Test set: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
